<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="ProgPrompt: Generating Situated Robot Task Plans using Large Language Models">
  <meta name="keywords" content="LLMs, Task Planning, Code Generation">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>ProgPrompt</title>

  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-179758052-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-179758052-1');
  </script>

  <script>
    function updateSingleVideo() {
      var demo = document.getElementById("single-menu-demos").value;
      var task = document.getElementById("single-menu-tasks").value;
      var inst = document.getElementById("single-menu-instances").value;

      console.log("single", demo, task, inst)

      var video = document.getElementById("multi-task-result-video");
      video.src = "media/results/sim_rollouts/" + 
                  "n" +
                  demo +
                  "-" +
                  task +
                  "-" +
                  inst +
                  ".mp4"
      video.playbackRate = 1.75;
      video.play();
    }

    function updateQpredVideo() {
      var task = document.getElementById("single-menu-qpred").value;

      console.log("qpred", task)

      var video = document.getElementById("q-pred-video");
      video.src = "media/results/qpred/" + 
                  task + 
                  ".mp4"
      video.playbackRate = 1.75;
      video.play();
    }

  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body onload="updateSingleVideo(); updateQpredVideo();">

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title"><span class="dprogprompt">ProgPrompt</span>: Generating Situated Robot Task Plans using Large Language Models</h1> 
          <h3 class="title is-4 conference-authors"><a target="_blank" href="https://www.icra2023.org/welcome">ICRA 2023</a></h3>
          <h3 class="title is-4 conference-authors">Extended version in <a target="_blank" href="https://link.springer.com/article/10.1007/s10514-023-10135-3">Autonomous Robots 2023</a></h3>
          <div class="is-size-4 publication-authors">
            <span class="author-block">
              <a target="_blank" href="https://ishikasingh.github.io/">Ishika Singh</a><sup>1</sup>,</span>
            <span class="author-block">
              <a target="_blank" href="https://www.cs.cornell.edu/~valts/">Valts Blukis</a><sup>2</sup>,</span>
            <span class="author-block">
              <a target="_blank" href="https://cs.gmu.edu/~amousavi/">Arsalan Mousavian</a><sup>2</sup>,</span>
            <span class="author-block">
              <a target="_blank" href="https://imankgoyal.github.io/">Ankit Goyal</a><sup>2</sup>,</span>
            <span class="author-block">
              <a target="_blank" href="https://faculty.cc.gatech.edu/~danfei/">Danfei Xu</a><sup>2</sup>,</span>
            <span class="author-block">
              <a target="_blank" href="https://research.nvidia.com/person/jonathan-tremblay">Jonathan Tremblay</a><sup>2</sup>,</span>
            <span class="author-block">
              <a target="_blank" href="https://homes.cs.washington.edu/~fox/">Dieter Fox</a><sup>2</sup>,</span>
            <span class="author-block">
              <a target="_blank" href="https://jessethomason.com/">Jesse Thomason</a><sup>1</sup>,</span>
            <span class="author-block">
              <a target="_blank" href="https://animesh.garg.tech/">Animesh Garg</a><sup>2</sup></span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>University of Southern California,</span>
            <span class="author-block"><sup>2</sup>NVIDIA</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a target="_blank" href="https://arxiv.org/abs/2209.11302"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>

            <!-- Video Link. -->
            <span class="link-block">
              <a target="_blank" href="https://youtu.be/cawHI_rYQoM"
                 class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                    <i class="fab fa-youtube"></i>
                </span>
                <span>Video</span>
              </a>
            </span>

            <!-- Code Link. -->
            <span class="link-block">
              <a target="_blank" href="https://github.com/NVlabs/progprompt-vh"
                 class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                    <i class="fab fa-github"></i>
                </span>
                <span>Code</span>
                </a>
            </span>

            <!-- poster Link. -->
            <span class="link-block">
              <a target="_blank" href="static/paper/poster.pdf"
                 class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                    <i class="fas fa-image"></i>
                </span>
                <span>Poster</span>
              </a>
            </span>

            <!-- FAQ Link. -->
            <span class="link-block">
              <a href="#faqs"
                 class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                    <i class="fas fa-question"></i>
                </span>
                <span>FAQs</span>
                </a>
            </span>

            

            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>



<!-- DO DiFF ENVS -->

<section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container has-text-centered">
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-steve">
          <h4 class="title is-6">Bring coffeepot and cupcake to the coffee table</h4>
          <video poster="" id="steve" autoplay muted loop playsinline height="100%">
            <source src="static/media/videos/task_unseen_bring coffeepot and cupcake to the coffee table.mov"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-coffee">
          <h4 class="title is-6">Sort fruits in the plate and bottle in the box</h4>
          <video poster="" id="coffee" autoplay muted loop playsinline height="100%">
            <source src="static/media/videos/real-robot.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-blueshirt">
          <h4 class="title is-6">Put salmon in the fridge</h4>
          <video poster="" id="blueshirt" autoplay muted loop playsinline height="100%">
            <source src="static/media/videos/task_unseen_putsalmoninthefridge.mov"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-mask">
          <h4 class="title is-6">Throw away apple</h4>
          <video poster="" id="mask" autoplay muted loop playsinline height="100%">
            <source src="static/media/videos/task_unseen_throw away apple.mov"
                    type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Task planning can require defining myriad domain knowledge 
            about the world in which a robot needs to act.
            To ameliorate that effort, large language models (LLMs) can be
            used to score potential next actions during task planning, and
            even generate action sequences directly, given an instruction in
            natural language with no additional domain information. However, 
            such methods either require enumerating all possible next
            steps for scoring, or generate free-form text that may contain
            actions not possible on a given robot in its current context. We
            present a programmatic LLM prompt structure that enables
            plan generation functional across situated environments, robot
            capabilities, and tasks. Our key insight is to prompt the LLM
            with program-like specifications of the available actions and
            objects in an environment, as well as with example programs
            that can be executed. We make concrete recommendations
            about prompt structure and generation constraints through
            ablation experiments, demonstrate state of the art success rates
            in VirtualHome household tasks, and deploy our method on a
            physical robot arm for tabletop tasks. 
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>

  <!-- Paper video. -->
  <div class="columns is-centered has-text-centered">
    <div class="column is-two-thirds">
      <h2 class="title is-3">Video</h2>
      <div class="publication-video">
        <!-- <iframe src="static/media/videos/recording_3min.mp4"
                frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe> -->
          <video poster="" id="mask" controls playsinline height="100%">
            <source src="static/media/videos/ICRA23_2372.mp4"
                    type="video/mp4">
          </video>
      </div>
    </div>
  </div>

</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- method. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3 dprogprompt">ProgPrompt</h2>
        <div class="content has-text-justified">
          <p>
            We introduce a prompting method that goes beyond conditioning LLMs in natural language, 
            utilizing programming language structures, leveraging the fact that LLMs
            are trained on several open-source codebases. <span class="dprogprompt">ProgPrompt</span>
            provides an LLM with a pythonic program header that imports available actions 
            and their arguments, shows a list of environment objects, 
            followed by multiple example task plans, formatted as pythonic functions. 
            The function name is the task specification, and the function implementation 
            is an example task plan. The plan consists of comments, actions, and assertions. 
            We use comments to group multiple high-level actions together, similar to chain-of-thought 
            reasoning. Actions are expressed as imported function calls. Assertions check for 
            action pre-conditions, and trigger recovery actions. Finally we append an incomplete 
            function definition for the LLM to complete. The plan is interpreted by executing actions 
            in the env and asserting preconditions using the LLM.
          </p>
        </div>
      </div>
    </div>
    <!--/ mothod. -->
  </div>

  <!-- Paper video. -->
  <div class="columns is-centered has-text-centered">
    <div class="column is-two-thirds">
      <!-- <h2 class="title is-3">Video</h2> -->
      <div class="publication-video">
        <video poster="" id="mask" autoplay muted loop playsinline height="100%">
          <source src="static/media/videos/method.mov"
                  type="video/mp4">
        </video>
      </div>
    </div>
  </div>

</section>

<section class="section">
  <div class="container is-max-widescreen">

    <div class="rows">


    <!-- Animation. -->
    <div class="rows is-centered has-text-centered">
      <div class="row is-full-width">
        <h2 class="title is-3"><span>Results</span></h2>
        <!-- <br>  -->
          
          <div class="container is-fullhd">
            <div class="hero-body">
              <div class="container">
                <h2 class="subtitle has-text-centered">
                  <span>Real Robot Demo</span> <br>
                  Task: sort fruits on the plate and bottle in the box
                  </h2>
                <div class="columns is-vcentered  is-centered">
                  <video id="teaser" autoplay muted loop height="100%">
                    <source src="static/media/videos/real-robot.mp4"
                            type="video/mp4">
                  </video>
                  <!-- </br> -->
                </div>
                <!-- <br> -->

              </div>
            </div>


            <div class="hero-body">
              <div class="container">
                <h2 class="subtitle has-text-centered">
                  <span>VirtualHome Demo </span> <br>
                  Task: microwave salmon
                  <!-- is an end-to-end behavior-cloning agent that can learn <br><b>a single language-conditioned policy</b> for 18 RLBench tasks with <b>249 unique task variations</b> -->
                  </h2>
                <div class="columns is-vcentered  is-centered">
                  
                  <video id="teaser" autoplay muted loop height="100%">
                    <source src="static/media/videos/VH_demo.mp4"
                            type="video/mp4">
                  </video>
                  <!-- </br> -->
                </div>
                <!-- <br> -->

              </div>
            </div>
          </div>

          <div class="hero-body">

            <div class="container">
              <h2 class="subtitle has-text-centered">
                <span>VirtualHome Results</span>
                <!-- is an end-to-end behavior-cloning agent that can learn <br><b>a single language-conditioned policy</b> for 18 RLBench tasks with <b>249 unique task variations</b> -->
                </h2>
              <table class="GeneratedTable padding-table-columns">
                <thead>
                </thead>
                <tbody>
                  <tr>
                    <th>bring coffeepot and cupcake to the coffee table</th>
                    <th>brush teeth</th>
                    <th>eat chips on the sofa</th>
                  </tr>
                  <tr>
                    <td>          
                      <video poster="" id="steve" autoplay="" muted="" loop="" playsinline="" height="100%" style="border-radius: 10px; ">
                        <source src="static/media/videos/task_unseen_bring coffeepot and cupcake to the coffee table.mov" type="video/mp4">
                    </video>
                  </td>
                    <td>          
                      <video poster="" id="steve" autoplay="" muted="" loop="" playsinline="" height="100%" style="border-radius: 10px; ">
                        <source src="static/media/videos/task_unseen_brush teeth.mov" type="video/mp4">
                    </video>
                  </td>
                    <td>          
                      <video poster="" id="steve" autoplay="" muted="" loop="" playsinline="" height="100%" style="border-radius: 10px; ">
                        <source src="static/media/videos/task_unseen_eatchipsonthesofa.mov" type="video/mp4">
                    </video>
                  </td>
                  </tr>
                  <tr>
                    <th>make toast</th>
                    <th>put salmon in the fridge</th>
                    <th>throw away apple</th>
                  </tr>
                  <tr>
                    <td>          
                      <video poster="" id="steve" autoplay="" muted="" loop="" playsinline="" height="100%" style="border-radius: 10px; ">
                        <source src="static/media/videos/task_unseen_make toast.mov" type="video/mp4">
                    </video>
                  </td>
                    <td>          
                      <video poster="" id="steve" autoplay="" muted="" loop="" playsinline="" height="100%" style="border-radius: 10px; ">
                        <source src="static/media/videos/task_unseen_putsalmoninthefridge.mov" type="video/mp4">
                    </video>
                  </td>
                    <td>          
                      <video poster="" id="steve" autoplay="" muted="" loop="" playsinline="" height="100%" style="border-radius: 10px; ">
                        <source src="static/media/videos/task_unseen_throw away apple.mov" type="video/mp4">
                    </video>
                  </td>
                  </tr>
                  <tr>
                    <th>turn off light</th>
                    <th>wash the plate.</th>
                    <th>watch tv</th>
                  </tr>
                  <tr>
                    <td>          
                      <video poster="" id="steve" autoplay="" muted="" loop="" playsinline="" height="100%" style="border-radius: 10px; ">
                        <source src="static/media/videos/task_unseen_turn off light.mov" type="video/mp4">
                    </video>
                  </td>
                    <td>          
                      <video poster="" id="steve" autoplay="" muted="" loop="" playsinline="" height="100%" style="border-radius: 10px; ">
                        <source src="static/media/videos/task_unseen_wash the plate.mov" type="video/mp4">
                    </video>
                  </td>
                    <td>          
                      <video poster="" id="steve" autoplay="" muted="" loop="" playsinline="" height="100%" style="border-radius: 10px; ">
                        <source src="static/media/videos/task_unseen_watch tv.mov" type="video/mp4">
                    </video>
                  </td>
                  </tr>
                </tbody>
              </table>
        
            </div>
          </div>

        <!-- Interpolating. -->
        <h2 class="subtitle has-text-centered">
          <span>Full Prompt</span>
          </h2>
        <object data="static/media/figures/full_prompt.pdf#toolbar=0&navpanes=0&scrollbar=0" width="100%" height="770px" 
            type="application/pdf"></object>

        
        <h2 class="subtitle has-text-centered">
          <span>Generated Task Programs</span>
          </h2>
        <object data="static/media/figures/task_egs.pdf#toolbar=0&navpanes=0&scrollbar=0" width="100%" height="1530px" 
            type="application/pdf"></object>

</section> 

<section class="section">
  <div class="container is-max-desktop">
    <!-- method. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-fifth-sixths">
        <h2 class="title is-3 dprogprompt"><a id="faqs">FAQs</a>

        </h2>
        <div class="content has-text-justified">

          <p><b>
          How does this approach compare with end-to-end robot learning models, and what are the current limitations?
           </b></p>  <p>
          <span class="dprogprompt">ProgPrompt</span> is a hierarchical solution to task planning where the abstract task descriptions leverage LLM’s reasoning and maps the task plan to the grounded environment labels. On the other hand, in end-to-end approaches, generally the model implicitly learns reasoning, planning, and grounding, while mapping the abstract task description to the action space directly.
          <br> 
              <b>Pros:</b><br> 
              <ol>              
                <li> LLMs can do long-horizon planning from an abstract task description.</li>
                <li> Decoupling the LLM planner from the environment makes generalization to new tasks and environments feasible. </li>
                <li> <span class="dprogprompt">ProgPrompt</span> enables LLMs to intelligently combine the robot capabilities with the environment and their own reasoning ability to generate an executable and valid task plan.</li>
                <li> The precondition checking helps recover from some failure modes that can happen if actions are generated in the wrong order or are missed by the base plan.</li>
              </ol>
              <br> 
              
              <b>Cons:</b><br> 
              <ol>    
                <li> Requires action space discretization, formalization of environments and objects.</li>
                <li> Plan generation is open-loop, with commonsense precondition checking-based environment interaction.</li>
                <li> Plan generation doesn't consider low-level continuous aspects of the environment state, and only reasons with the semantic state for planning as well as precondition checking. </li>
                <li> The amount of information exchange between language models and other modules such as the robot's perceptual or proprioceptive state encoders is limited, since API-based access to these recent LLMs only allows textual queries. However, this is still promising as it indicates the need for a multimodal encoder that can work with input such as vision, touch, force, temperature, etc.</li>
              </ol>    

           </p> <p><b>
            How does it compare with the concurrent work: Code-as-Policies (CaP)?  
           </b></p>  <p>
          <ol>
              <li> We believe that the general approach is quite similar to ours. CaP defines Hints and Examples which may correspond to Imports/Object lists and Task Plan examples in <span class="dprogprompt">ProgPrompt</span> .</li>
              <li> CaP uses actions as API calls with certain parameters for the calls such as robot arm pose, velocity, etc. We use actions as API calls with objects as parameters.</li>
              <li> CaP uses APIs to obtain environment information as well, like object pose or segmentation, for the purpose of plan generation. However, <span class="dprogprompt">ProgPrompt</span> extracts environment information via precondition checking on current environment state, to ensure plan executability. <span class="dprogprompt">ProgPrompt</span> also generates the prompt conditioned on information from perception models.</li>
          </ol>

           </p> <p><b>
              During ``PROMPT for State Feedback”, it seems that the prompt already includes all information about the environment state. Is it necessary to prompt the LLM again for the assertion (compared to a simple rule-based algorithm)?
           </b></p>  <p>
          <ol>
              <li> The environment state input to the model is not the full state for brevity. Thus, checking pre-conditions with the full state separately helps, as shown in Table 1 in the paper.</li>
              <li> The environment state could change during execution.</li>
              <li> Using LLM as opposed to a rule-based algorithm is a design choice made to keep the approach more general, instead of using a hand-coded rule-based algorithm. The assertion checking may also be replaced with a visual state conditioned module, when a semantic state is not available, such as in the real-world scenario. However, we leave these aspects to be addressed in future research. </li>
          </ol>

           </p> <p><b>
              Is it possible that the generated code will lead the robot to be stuck in an infinite loop?
           </b></p>  <p>
          LLM code generation could lead to loops by predicting the same actions repeatedly as a generation artifact. LLMs used to suffer from such degeneration, but with latest LLMs (i.e. GPT-3) we have not encountered it at all.

           </p> <p><b>
              Why are real-robot experiments simpler than virtual experiments?
           </b></p>  <p>
          The real-robot experiments were done as a demonstration of the approach on a real-robot, while studying the method in depth in a virtual simulator, for the sake of simplicity and efficiency.

           </p> <p><b>
              What’s the difference between various GPT3 model versions used in this project?
           </b></p>  <p>
          We name 'GPT3', which is the latest available version of GPT3 model on OpenAI at the time the paper was written: 'text-davinci-002'. We name 'davinci' as the original version of GPT3 released: 'text-davinci'. More info on GPT3 models variations and naming can be found <a href="https://platform.openai.com/docs/models/overview">here</a>.

           </p> <p><b>
              Why not a planning language like PDDL (or other planning languages) be used to construct <span class="dprogprompt">ProgPrompt</span>? Any advantages of using a pythonic structure?
           </b></p>  <p>
          <ol>
              <li> GPT-3 has been trained on data from the internet. There is a lot of python code on the internet, while PDDL is a language of much more narrow interest. Thus, we expect the LLM to better understand python syntax.</li>
              <li> Python is a general purpose language, so it has more features than PDDL. Furthermore, we want to avoid specifying the full planning domain, instead relying on the knowledge learned by the LLM to make common-sense inferences. A recent work (<a href="https://arxiv.org/abs/2302.05128">Translating Natural Language to Planning Goals with Large-Language Models</a>) uses LLMs to generate PDDL goals, however, it requires full domain specification for a given environment.</li>
              <li> Python is an accessible language that a larger community is familiar with.</li>
          </ol>

           </p> <p><b>
              How to handle multiple instances of the same object type in the scene?
           </b></p>  <p>
          <span class="dprogprompt">ProgPrompt</span> doesn’t tackle the issue, however, <a href="https://arxiv.org/abs/2302.05128">Translating Natural Language to Planning Goals with Large-Language Models</a> shows that multiple instances of the same objects can be handled by using labels with object IDs such as 'book_1, book_2'.

           </p> <p><b>
              Why doesn't the paper compare the performance of the proposed method to InnerMonologue, SAYCAN, or Socratic models?
           </b></p>  <p>
          At the time of writing, the dataset or model from the above papers were not public. However, we do compare with a proxy approach, similar in underlying idea to the above approaches, in the VirtualHome environment. 'LangPlan' in our baselines, uses GPT3 to get textual plan steps, which are then executed using a GPT-2 based trained policy.

           </p> <p><b>
              So the next step in this direction of research is to create highly structured inputs and outputs that could be compiled, since eventually we want something that compiles on robotic machines?
           </b></p>  <p>
          The disconnect and information bottleneck between LLM planning module and skill execution module might make it less concrete on “how much” and “what” information should be passed through the LLM during planning. That said, we think that this would be an interesting direction to pursue and test the limits of LLM’s highly structured input understanding and generation.

           </p> <p><b>
              How does it compare to a classical planner?
           </b></p>  <p>
          <ol>
              <li> Classical planners require concrete goal condition specification. An LLM planner reasons out a feasible goal state from a high level task description, such as “microwave salmon”. From a user’s perspective, it is desirable to not have to specify a concrete semantic goal state of the environment and just be able to give an instruction to act on. </li>
              <li> The search space would also be huge without common sense priors that an LLM planner leverages as opposed to a classical planner. Moreover, we also bypass the need to specify the domain knowledge needed for the search to roll out. </li>
              <li> Moreover, the domain specification and search space will grow non-linearly with the complexity of the environment.</li>
          </ol>

           </p> <p><b>
              Is it possible to decouple high-level language planning from low-level perceptual planning?
           </b></p>  <p>
          It may be feasible to an extent, however we believe that a clean decoupling might not be “all we need”. For instance, imagine an agent being stuck at an action that needs to be resolved at semantic level of reasoning, and probably very hard for the visual module to figure out. For instance, while placing a dish on an oven tray, the robot may need to pull the dish rack out of the oven to be successful in the task. 

           </p> <p><b>
              What are the kinds of failures that can happen with <span class="dprogprompt">ProgPrompt</span>-like 2 stage decoupled pipeline?
           </b></p>  <p>
          A few broad failure categories could be:
          <ol>
              <li> Generation of a semantically wrong action.</li>
              <li> Robot might fail to execute the action at perception/action/skill level.</li>
              <li> Robot needs to recover from a failure by taking a different high-level action, i.e., a precondition needs to be satisfied. The challenge is to identify that precondition from the current state of the environment and the agent.</li>
          </ol>

        </div>
      </div>
    </div>
    <!--/ mothod. -->
  </div>

</section>


<section class="section" id="BibTeX">
  <div class="container is-max-widescreen content">
    <h2 class="title">BibTeX</h2>
 <pre><code>@INPROCEEDINGS{10161317,
  author={Singh, Ishika and Blukis, Valts and Mousavian, Arsalan and Goyal, Ankit and Xu, Danfei and Tremblay, Jonathan and Fox, Dieter and Thomason, Jesse and Garg, Animesh},
  booktitle={2023 IEEE International Conference on Robotics and Automation (ICRA)}, 
  title={ProgPrompt: Generating Situated Robot Task Plans using Large Language Models}, 
  year={2023},
  volume={},
  number={},
  pages={11523-11530},
  doi={10.1109/ICRA48891.2023.10161317}}
</code></pre> 
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column">
        <div class="content has-text-centered">
          <p>
            Website template borrowed from <a href="https://github.com/nerfies/nerfies.github.io">NeRFies</a> and <a href="https://github.com/peract/peract.github.io">PerAct</a>
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>


</body>
</html>
