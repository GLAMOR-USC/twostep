<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="ProgPrompt: Generating Situated Robot Task Plans using Large Language Models">
  <meta name="keywords" content="LLMs, Task Planning, Code Generation">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>ProgPrompt</title>

  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-179758052-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-179758052-1');
  </script>

  <script>
    function updateSingleVideo() {
      var demo = document.getElementById("single-menu-demos").value;
      var task = document.getElementById("single-menu-tasks").value;
      var inst = document.getElementById("single-menu-instances").value;

      console.log("single", demo, task, inst)

      var video = document.getElementById("multi-task-result-video");
      video.src = "media/results/sim_rollouts/" + 
                  "n" +
                  demo +
                  "-" +
                  task +
                  "-" +
                  inst +
                  ".mp4"
      video.playbackRate = 1.75;
      video.play();
    }

    function updateQpredVideo() {
      var task = document.getElementById("single-menu-qpred").value;

      console.log("qpred", task)

      var video = document.getElementById("q-pred-video");
      video.src = "media/results/qpred/" + 
                  task + 
                  ".mp4"
      video.playbackRate = 1.75;
      video.play();
    }

  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body onload="updateSingleVideo(); updateQpredVideo();">

<!-- <nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" target="_blank" href="https://ishikasingh.github.io">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" target="_blank" href="https://cliport.github.io">
            CLIPort
          </a>
          <a class="navbar-item" target="_blank" href="https://askforalfred.com/">
            ALFRED
          </a>
          <a class="navbar-item" target="_blank" href="http://alfworld.github.io/">
            ALFWorld
          </a>
          <a class="navbar-item" target="_blank" href="https://arxiv.org/pdf/1806.03831.pdf">
            INGRESS
          </a>
        </div>
      </div>
    </div>
  </div>
</nav> -->

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title"><span class="dprogprompt">ProgPrompt</span>: Generating Situated Robot Task Plans using Large Language Models</h1> 
          <h3 class="title is-4 conference-authors"><a target="_blank" href="https://www.icra2023.org/welcome">ICRA 2023</a></h3>
          <div class="is-size-4 publication-authors">
            <span class="author-block">
              <a target="_blank" href="https://ishikasingh.github.io/">Ishika Singh</a><sup>1</sup>,</span>
            <span class="author-block">
              <a target="_blank" href="https://www.cs.cornell.edu/~valts/">Valts Blukis</a><sup>2</sup>,</span>
            <span class="author-block">
              <a target="_blank" href="https://cs.gmu.edu/~amousavi/">Arsalan Mousavian</a><sup>2</sup>,</span>
            <span class="author-block">
              <a target="_blank" href="https://imankgoyal.github.io/">Ankit Goyal</a><sup>2</sup>,</span>
            <span class="author-block">
              <a target="_blank" href="https://faculty.cc.gatech.edu/~danfei/">Danfei Xu</a><sup>2</sup>,</span>
            <span class="author-block">
              <a target="_blank" href="https://research.nvidia.com/person/jonathan-tremblay">Jonathan Tremblay</a><sup>2</sup>,</span>
            <span class="author-block">
              <a target="_blank" href="https://homes.cs.washington.edu/~fox/">Dieter Fox</a><sup>2</sup>,</span>
            <span class="author-block">
              <a target="_blank" href="https://jessethomason.com/">Jesse Thomason</a><sup>1</sup>,</span>
            <span class="author-block">
              <a target="_blank" href="https://animesh.garg.tech/">Animesh Garg</a><sup>2</sup></span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>University of Southern California,</span>
            <span class="author-block"><sup>2</sup>NVIDIA</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a target="_blank" href="https://arxiv.org/abs/2209.11302"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>

            <!-- Video Link. -->
            <span class="link-block">
              <a target="_blank" href="https://youtu.be/cawHI_rYQoM"
                 class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                    <i class="fab fa-youtube"></i>
                </span>
                <span>Video</span>
              </a>
            </span>

            <!-- Code Link. -->
            <span class="link-block">
              <a target="_blank" href="https://github.com/progprompt/progprompt"
                 class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                    <i class="fab fa-github"></i>
                </span>
                <span>Code (soon)</span>
                </a>
            </span>

            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>



<!-- DO DiFF ENVS -->

<section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container has-text-centered">
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-steve">
          <h4 class="title is-6">Bring coffeepot and cupcake to the coffee table</h4>
          <video poster="" id="steve" autoplay muted loop playsinline height="100%">
            <source src="static/media/videos/task_unseen_bring coffeepot and cupcake to the coffee table.mov"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-coffee">
          <h4 class="title is-6">Sort fruits in the plate and bottle in the box</h4>
          <video poster="" id="coffee" autoplay muted loop playsinline height="100%">
            <source src="static/media/videos/real-robot.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-blueshirt">
          <h4 class="title is-6">Put salmon in the fridge</h4>
          <video poster="" id="blueshirt" autoplay muted loop playsinline height="100%">
            <source src="static/media/videos/task_unseen_putsalmoninthefridge.mov"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-mask">
          <h4 class="title is-6">Throw away apple</h4>
          <video poster="" id="mask" autoplay muted loop playsinline height="100%">
            <source src="static/media/videos/task_unseen_throw away apple.mov"
                    type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- <h2 class="subtitle has-text-centered">
</br>
  We also train <b>one multi-task Transformer from scratch</b> on 7 real-world tasks with just <b>53 demos</b> in total.
</h2> -->


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Task planning can require defining myriad domain knowledge 
            about the world in which a robot needs to act.
            To ameliorate that effort, large language models (LLMs) can be
            used to score potential next actions during task planning, and
            even generate action sequences directly, given an instruction in
            natural language with no additional domain information. However, 
            such methods either require enumerating all possible next
            steps for scoring, or generate free-form text that may contain
            actions not possible on a given robot in its current context. We
            present a programmatic LLM prompt structure that enables
            plan generation functional across situated environments, robot
            capabilities, and tasks. Our key insight is to prompt the LLM
            with program-like specifications of the available actions and
            objects in an environment, as well as with example programs
            that can be executed. We make concrete recommendations
            about prompt structure and generation constraints through
            ablation experiments, demonstrate state of the art success rates
            in VirtualHome household tasks, and deploy our method on a
            physical robot arm for tabletop tasks. 
          </p>
          <!-- <p>
            We investigate this question with <span class="dperact">PerAct</span>, a
            <span class="dperact">PerAct</span> encodes language goals and RGB-D voxel observations with a 
            <a target=”_blank” href="https://www.deepmind.com/blog/building-architectures-that-can-handle-the-worlds-data">Perceiver Transformer</a>, and outputs discretized 
          </p> -->
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>

  <!-- Paper video. -->
  <div class="columns is-centered has-text-centered">
    <div class="column is-two-thirds">
      <h2 class="title is-3">Video</h2>
      <div class="publication-video">
        <!-- <iframe src="static/media/videos/recording_3min.mp4"
                frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe> -->
          <video poster="" id="mask" controls playsinline height="100%">
            <source src="static/media/videos/recording_3min.mp4"
                    type="video/mp4">
          </video>
      </div>
    </div>
  </div>

</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- method. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3 dprogprompt">ProgPrompt</h2>
        <div class="content has-text-justified">
          <p>
            We introduce a prompting method that goes beyond conditioning LLMs in natural language, 
            utilizing programming language structures, leveraging the fact that LLMs
            are trained on several open-source codebases. <span class="dprogprompt">ProgPrompt</span>
            provides an LLM with a pythonic program header that imports available actions 
            and their arguments, shows a list of environment objects, 
            followed by multiple example task plans, formatted as pythonic functions. 
            The function name is the task specification, and the function implementation 
            is an example task plan. The plan consists of comments, actions, and assertions. 
            We use comments to group multiple high-level actions together, similar to chain-of-thought 
            reasoning. Actions are expressed as imported function calls. Assertions check for 
            action pre-conditions, and trigger recovery actions. Finally we append an incomplete 
            function definition for the LLM to complete. The plan is interpreted by executing actions 
            in the env and asserting preconditions using the LLM.
          </p>
        </div>
      </div>
    </div>
    <!--/ mothod. -->
  </div>

  <!-- Paper video. -->
  <div class="columns is-centered has-text-centered">
    <div class="column is-two-thirds">
      <!-- <h2 class="title is-3">Video</h2> -->
      <div class="publication-video">
        <video poster="" id="mask" autoplay muted loop playsinline height="100%">
          <source src="static/media/videos/method.mov"
                  type="video/mp4">
        </video>
      </div>
    </div>
  </div>

</section>

<section class="section">
  <div class="container is-max-widescreen">

    <div class="rows">


    <!-- Animation. -->
    <div class="rows is-centered has-text-centered">
      <div class="row is-full-width">
        <h2 class="title is-3"><span>Results</span></h2>
        <!-- <br>  -->
          
          <div class="container is-fullhd">
            <div class="hero-body">
              <div class="container">
                <h2 class="subtitle has-text-centered">
                  <span>Real Robot Demo</span> <br>
                  Task: sort fruits on the plate and bottle in the box
                  <!-- is an end-to-end behavior-cloning agent that can learn <br><b>a single language-conditioned policy</b> for 18 RLBench tasks with <b>249 unique task variations</b> -->
                  </h2>
                <div class="columns is-vcentered  is-centered">
                  <video id="teaser" autoplay muted loop height="100%">
                    <source src="static/media/videos/real-robot.mp4"
                            type="video/mp4">
                  </video>
                  <!-- </br> -->
                </div>
                <!-- <br> -->

              </div>
            </div>


            <div class="hero-body">
              <div class="container">
                <h2 class="subtitle has-text-centered">
                  <span>VirtualHome Demo </span> <br>
                  Task: microwave salmon
                  <!-- is an end-to-end behavior-cloning agent that can learn <br><b>a single language-conditioned policy</b> for 18 RLBench tasks with <b>249 unique task variations</b> -->
                  </h2>
                <div class="columns is-vcentered  is-centered">
                  
                  <video id="teaser" autoplay muted loop height="100%">
                    <source src="static/media/videos/VH_demo.mp4"
                            type="video/mp4">
                  </video>
                  <!-- </br> -->
                </div>
                <!-- <br> -->

              </div>
            </div>
          </div>

          <div class="hero-body">

            <div class="container">
              <h2 class="subtitle has-text-centered">
                <span>VirtualHome Results</span>
                <!-- is an end-to-end behavior-cloning agent that can learn <br><b>a single language-conditioned policy</b> for 18 RLBench tasks with <b>249 unique task variations</b> -->
                </h2>
              <table class="GeneratedTable padding-table-columns">
                <thead>
                </thead>
                <tbody>
                  <tr>
                    <th>bring coffeepot and cupcake to the coffee table</th>
                    <th>brush teeth</th>
                    <th>eat chips on the sofa</th>
                  </tr>
                  <tr>
                    <td>          
                      <video poster="" id="steve" autoplay="" muted="" loop="" playsinline="" height="100%" style="border-radius: 10px; ">
                        <source src="static/media/videos/task_unseen_bring coffeepot and cupcake to the coffee table.mov" type="video/mp4">
                    </video>
                  </td>
                    <td>          
                      <video poster="" id="steve" autoplay="" muted="" loop="" playsinline="" height="100%" style="border-radius: 10px; ">
                        <source src="static/media/videos/task_unseen_brush teeth.mov" type="video/mp4">
                    </video>
                  </td>
                    <td>          
                      <video poster="" id="steve" autoplay="" muted="" loop="" playsinline="" height="100%" style="border-radius: 10px; ">
                        <source src="static/media/videos/task_unseen_eatchipsonthesofa.mov" type="video/mp4">
                    </video>
                  </td>
                  </tr>
                  <tr>
                    <th>make toast</th>
                    <th>put salmon in the fridge</th>
                    <th>throw away apple</th>
                  </tr>
                  <tr>
                    <td>          
                      <video poster="" id="steve" autoplay="" muted="" loop="" playsinline="" height="100%" style="border-radius: 10px; ">
                        <source src="static/media/videos/task_unseen_make toast.mov" type="video/mp4">
                    </video>
                  </td>
                    <td>          
                      <video poster="" id="steve" autoplay="" muted="" loop="" playsinline="" height="100%" style="border-radius: 10px; ">
                        <source src="static/media/videos/task_unseen_putsalmoninthefridge.mov" type="video/mp4">
                    </video>
                  </td>
                    <td>          
                      <video poster="" id="steve" autoplay="" muted="" loop="" playsinline="" height="100%" style="border-radius: 10px; ">
                        <source src="static/media/videos/task_unseen_throw away apple.mov" type="video/mp4">
                    </video>
                  </td>
                  </tr>
                  <tr>
                    <th>turn off light</th>
                    <th>wash the plate.</th>
                    <th>watch tv</th>
                  </tr>
                  <tr>
                    <td>          
                      <video poster="" id="steve" autoplay="" muted="" loop="" playsinline="" height="100%" style="border-radius: 10px; ">
                        <source src="static/media/videos/task_unseen_turn off light.mov" type="video/mp4">
                    </video>
                  </td>
                    <td>          
                      <video poster="" id="steve" autoplay="" muted="" loop="" playsinline="" height="100%" style="border-radius: 10px; ">
                        <source src="static/media/videos/task_unseen_wash the plate.mov" type="video/mp4">
                    </video>
                  </td>
                    <td>          
                      <video poster="" id="steve" autoplay="" muted="" loop="" playsinline="" height="100%" style="border-radius: 10px; ">
                        <source src="static/media/videos/task_unseen_watch tv.mov" type="video/mp4">
                    </video>
                  </td>
                  </tr>
                </tbody>
              </table>
        
            </div>
          </div>

        <!-- Interpolating. -->
        <h2 class="subtitle has-text-centered">
          <span>Full Prompt</span>
          <!-- is an end-to-end behavior-cloning agent that can learn <br><b>a single language-conditioned policy</b> for 18 RLBench tasks with <b>249 unique task variations</b> -->
          </h2>
        <!-- <div class="content has-text-justified">
        </div>
        <p>
          <span class="dperact">PerAct</span> is a language-conditioned behavior-cloning agent trained with supervised learning to <i>detect actions</i>. Instead of using object-detectors, instance-segmentors, or pose-estimators to represent a scene and then learning a policy, <span class="dperact">PerAct</span> directly learns <b>perceptual representations of actions</b> conditioned on language goals. This <a target="_blank" href="https://en.wikipedia.org/wiki/Ecological_psychology">action-centric approach</a> with a unified observation and action space makes <span class="dperact">PerAct</span> applicable to a broad range of tasks involving articulated objects, deformable objects, granular media, and even some non-prehensile interactions with tools.  
        </p>
        </br>
        </br> -->
        <object data="static/media/figures/full_prompt.pdf#toolbar=0&navpanes=0&scrollbar=0" width="100%" height="770px" 
            type="application/pdf"></object>

        
        <h2 class="subtitle has-text-centered">
          <span>Generated Task Programs</span>
          <!-- is an end-to-end behavior-cloning agent that can learn <br><b>a single language-conditioned policy</b> for 18 RLBench tasks with <b>249 unique task variations</b> -->
          </h2>
        <!-- <div class="content has-text-justified">
        </div>
        <p>
          <span class="dperact">PerAct</span> is a language-conditioned behavior-cloning agent trained with supervised learning to <i>detect actions</i>. Instead of using object-detectors, instance-segmentors, or pose-estimators to represent a scene and then learning a policy, <span class="dperact">PerAct</span> directly learns <b>perceptual representations of actions</b> conditioned on language goals. This <a target="_blank" href="https://en.wikipedia.org/wiki/Ecological_psychology">action-centric approach</a> with a unified observation and action space makes <span class="dperact">PerAct</span> applicable to a broad range of tasks involving articulated objects, deformable objects, granular media, and even some non-prehensile interactions with tools.  
        </p>
        </br>
        </br> -->
        <object data="static/media/figures/task_egs.pdf#toolbar=0&navpanes=0&scrollbar=0" width="100%" height="1530px" 
            type="application/pdf"></object>

</section> 


<section class="section" id="BibTeX">
  <div class="container is-max-widescreen content">
    <h2 class="title">BibTeX</h2>
 <pre><code>@article{singh2022progprompt,
  title={{ProgPrompt}: Generating Situated Robot Task Plans using Large Language Models}, 
  author={Ishika Singh and Valts Blukis and Arsalan Mousavian and Ankit Goyal and Danfei Xu and Jonathan Tremblay and Dieter Fox and Jesse Thomason and Animesh Garg},
  year={2022},
  eprint={2209.11302},
  archivePrefix={arXiv},
  primaryClass={cs.RO}
}</code></pre> 
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column">
        <div class="content has-text-centered">
          <p>
            Website template borrowed from <a href="https://github.com/nerfies/nerfies.github.io">NeRFies</a> made by the amazing <a href="https://keunhong.com/">Keunhong Park</a>. 
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>


</body>
</html>
